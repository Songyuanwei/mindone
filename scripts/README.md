# Utility Scripts

This folder is a collection of utility scripts, listed and explained below.

> All scripts need to be run in the root path of project, unless otherwise noted.

## eval_videos_metrics.py

This script contains code and scripts for diffusion model evaluation, e.g.,

- CLIP Score For frame Consistency
- CLIP Score for Textual Alignment


Note that all the above metrics are computed based on neural network models.

> A convincing evaluation for diffusion models requires both visually qualitative comparision and quantitative measure. A higher CLIP score does not necessarily show one model is better than another.


#### CLIP Score for Frame Consistency

To compute the CLIP score on all frames of output video and report the average cosine similarity between all video frame pairs, please run

```shell
python ./scripts/eval_videos_metrics.py --video_data_dir <path-to-video-dir> --video_caption_path <path-to-video-caption-path> --model_name <HF-model-name>  --metric clip_score_frame
```

#### CLIP Score for Textual Alignment

To compute the average CLIP score between all frames of the output video and the corresponding editing prompts, please run

```shell
python ./scripts/eval_videos_metrics.py --video_data_dir <path-to-video-dir> --video_caption_path <path-to-video-caption-path> --model_name <HF-model-name>  --metric clip_score_text
```

Format of `.csv`:
```
video,caption
video_name1.mp4,"an airliner is taxiing on the tarmac at Dubai Airport"
video_name2.mp4,"a pigeon sitting on the street near the house"
...
```



## eval_safety_checker.py

To validate the safeness of images or videos generated by a model, we implement two safety checkers in `safety_checker.py` with `safety_version=1` and `safety_version=2`(Default).

`safety_version=1` applies CLIP to compute the similarity between the given images and a pre-defined list of NSFW concepts (two possible `.yaml` files to choose from). This is consistent with the one used in CompVis's stable diffusion 1.x from `diffusers` implementation. The output is a list of True or False with bad concepts.

`safety_version=2` trains a NSFW classifier with a supervised approach, taking image features from CLIP as its input. This is consistent with the one used in StabilityAI's stable diffusion 2.x. The output is a number in 0-1, representing the probability of the generated image being NSFW.
> Note: when `safety_version=2`, safety checker currently only supports `openai/clip-vit-large-patch14`.

#### Image Safety Check

```
python ./scripts/eval_safety_checker.py --image_path_or_dir <path-to-image>
```

### Video Safety Check

In video safety checks, keyframes are extracted from the video and then subjected to potential NSFW content detection.

```
python ./scripts/eval_safety_checker.py --video_path_or_dir <path-to-image>
```

For more usage, please run `python ./scripts/eval_safety_checker.py -h`.

## Reference

[1] https://github.com/showlab/loveu-tgve-2023/tree/main
[2] https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py
[3] https://github.com/LAION-AI/CLIP-based-NSFW-Detector
